# parsers/announcement_parser.py
"""
–£–º–Ω—ã–π –ø–∞—Ä—Å–µ—Ä –∞–Ω–æ–Ω—Å–æ–≤ —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π —Ä–∞–∑–ª–∏—á–Ω—ã—Ö —Å—Ç—Ä–∞—Ç–µ–≥–∏–π –æ—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏—è –∏–∑–º–µ–Ω–µ–Ω–∏–π
+ –ü–æ–¥–¥–µ—Ä–∂–∫–∞ –±—Ä–∞—É–∑–µ—Ä–Ω–æ–≥–æ –ø–∞—Ä—Å–∏–Ω–≥–∞ –¥–ª—è –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–∏—Ö —Å—Ç—Ä–∞–Ω–∏—Ü —Å JavaScript
"""
import re
import hashlib
import logging
from typing import List, Dict, Any, Optional
from bs4 import BeautifulSoup
from .base_parser import BaseParser

logger = logging.getLogger(__name__)


class AnnouncementParser(BaseParser):
    """
    –ü–∞—Ä—Å–µ—Ä –∞–Ω–æ–Ω—Å–æ–≤ —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π —Ä–∞–∑–ª–∏—á–Ω—ã—Ö —Å—Ç—Ä–∞—Ç–µ–≥–∏–π:
    - any_change: –û—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏–µ –ª—é–±—ã—Ö –∏–∑–º–µ–Ω–µ–Ω–∏–π –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ
    - element_change: –û—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏–µ –∏–∑–º–µ–Ω–µ–Ω–∏–π –≤ –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–º —ç–ª–µ–º–µ–Ω—Ç–µ (CSS Selector)
    - any_keyword: –ü–æ–∏—Å–∫ –ª—é–±–æ–≥–æ –∏–∑ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤
    - all_keywords: –í—Å–µ –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –¥–æ–ª–∂–Ω—ã –ø—Ä–∏—Å—É—Ç—Å—Ç–≤–æ–≤–∞—Ç—å
    - regex: –ü–æ–∏—Å–∫ –ø–æ —Ä–µ–≥—É–ª—è—Ä–Ω–æ–º—É –≤—ã—Ä–∞–∂–µ–Ω–∏—é
    """

    def __init__(self, url: str):
        super().__init__(url)
        self.strategies = {
            'any_change': self._strategy_any_change,
            'element_change': self._strategy_element_change,
            'any_keyword': self._strategy_any_keyword,
            'all_keywords': self._strategy_all_keywords,
            'regex': self._strategy_regex
        }

    def parse(
        self,
        strategy: str,
        last_snapshot: Optional[str] = None,
        keywords: Optional[List[str]] = None,
        regex_pattern: Optional[str] = None,
        css_selector: Optional[str] = None,
        use_browser: bool = False
    ) -> Dict[str, Any]:
        """
        –ü–∞—Ä—Å–∏–Ω–≥ –∞–Ω–æ–Ω—Å–æ–≤ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –≤—ã–±—Ä–∞–Ω–Ω–æ–π —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏

        Args:
            strategy: –ù–∞–∑–≤–∞–Ω–∏–µ —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏ ('any_change', 'element_change', 'any_keyword', 'all_keywords', 'regex')
            last_snapshot: –ü—Ä–µ–¥—ã–¥—É—â–∏–π —Å–Ω–∏–º–æ–∫ (hash –∏–ª–∏ —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ)
            keywords: –°–ø–∏—Å–æ–∫ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤ (–¥–ª—è —Å—Ç—Ä–∞—Ç–µ–≥–∏–π any_keyword –∏ all_keywords)
            regex_pattern: –†–µ–≥—É–ª—è—Ä–Ω–æ–µ –≤—ã—Ä–∞–∂–µ–Ω–∏–µ (–¥–ª—è —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏ regex)
            css_selector: CSS —Å–µ–ª–µ–∫—Ç–æ—Ä (–¥–ª—è —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏ element_change)
            use_browser: –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –±—Ä–∞—É–∑–µ—Ä–Ω—ã–π –ø–∞—Ä—Å–µ—Ä (Playwright) –¥–ª—è –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–∏—Ö —Å—Ç—Ä–∞–Ω–∏—Ü

        Returns:
            {
                'changed': bool,  # –ë—ã–ª–∏ –ª–∏ –∏–∑–º–µ–Ω–µ–Ω–∏—è
                'new_snapshot': str,  # –ù–æ–≤—ã–π —Å–Ω–∏–º–æ–∫ –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è
                'matched_content': str,  # –ù–∞–π–¥–µ–Ω–Ω—ã–π –∫–æ–Ω—Ç–µ–Ω—Ç (–µ—Å–ª–∏ –µ—Å—Ç—å)
                'message': str  # –°–æ–æ–±—â–µ–Ω–∏–µ –æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–µ
            }
        """
        try:
            logger.info(f"üîç AnnouncementParser: –ù–∞—á–∞–ª–æ –ø–∞—Ä—Å–∏–Ω–≥–∞")
            logger.info(f"   URL: {self.url}")
            logger.info(f"   –°—Ç—Ä–∞—Ç–µ–≥–∏—è: {strategy}")
            logger.info(f"   –ë—Ä–∞—É–∑–µ—Ä–Ω—ã–π –ø–∞—Ä—Å–µ—Ä: {'‚úÖ –î–ê' if use_browser else '‚ùå –ù–ï–¢'}")

            # –ü—Ä–æ–≤–µ—Ä–∫–∞ –≤–∞–ª–∏–¥–Ω–æ—Å—Ç–∏ —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏
            if strategy not in self.strategies:
                logger.error(f"‚ùå –ù–µ–∏–∑–≤–µ—Å—Ç–Ω–∞—è —Å—Ç—Ä–∞—Ç–µ–≥–∏—è: {strategy}")
                return {
                    'changed': False,
                    'new_snapshot': last_snapshot,
                    'matched_content': None,
                    'message': f"–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–∞—è —Å—Ç—Ä–∞—Ç–µ–≥–∏—è: {strategy}"
                }

            # –ü–æ–ª—É—á–∞–µ–º HTML —Å—Ç—Ä–∞–Ω–∏—Ü—ã (–±—Ä–∞—É–∑–µ—Ä–Ω—ã–π –∏–ª–∏ –æ–±—ã—á–Ω—ã–π –ø–∞—Ä—Å–∏–Ω–≥)
            if use_browser:
                logger.info(f"üåê –ò—Å–ø–æ–ª—å–∑—É–µ–º –±—Ä–∞—É–∑–µ—Ä–Ω—ã–π –ø–∞—Ä—Å–µ—Ä (Playwright)")
                html_content = self._fetch_with_browser()
                
                if not html_content:
                    logger.error(f"‚ùå –ë—Ä–∞—É–∑–µ—Ä–Ω—ã–π –ø–∞—Ä—Å–µ—Ä –Ω–µ —Å–º–æ–≥ –∑–∞–≥—Ä—É–∑–∏—Ç—å —Å—Ç—Ä–∞–Ω–∏—Ü—É")
                    return {
                        'changed': False,
                        'new_snapshot': last_snapshot,
                        'matched_content': None,
                        'message': "–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å —Å—Ç—Ä–∞–Ω–∏—Ü—É —á–µ—Ä–µ–∑ –±—Ä–∞—É–∑–µ—Ä"
                    }
            else:
                logger.debug(f"üì° –ó–∞–≥—Ä—É–∑–∫–∞ HTML —Å—Ç—Ä–∞–Ω–∏—Ü—ã —á–µ—Ä–µ–∑ HTTP...")
                response = self.make_request(self.url, timeout=(10, 30))

                if not response:
                    logger.error(f"‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å —Å—Ç—Ä–∞–Ω–∏—Ü—É")
                    return {
                        'changed': False,
                        'new_snapshot': last_snapshot,
                        'matched_content': None,
                        'message': "–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å —Å—Ç—Ä–∞–Ω–∏—Ü—É"
                    }

                response.raise_for_status()
                html_content = response.text
            
            logger.info(f"‚úÖ HTML –∑–∞–≥—Ä—É–∂–µ–Ω ({len(html_content)} –±–∞–π—Ç)")

            # –ü–∞—Ä—Å–∏–º HTML
            soup = BeautifulSoup(html_content, 'html.parser')

            # –í—ã–ø–æ–ª–Ω—è–µ–º –≤—ã–±—Ä–∞–Ω–Ω—É—é —Å—Ç—Ä–∞—Ç–µ–≥–∏—é
            strategy_func = self.strategies[strategy]
            result = strategy_func(
                soup=soup,
                html_content=html_content,
                last_snapshot=last_snapshot,
                keywords=keywords,
                regex_pattern=regex_pattern,
                css_selector=css_selector,
                use_browser=use_browser  # –ü–µ—Ä–µ–¥–∞–µ–º —Ñ–ª–∞–≥ –±—Ä–∞—É–∑–µ—Ä–Ω–æ–≥–æ –ø–∞—Ä—Å–∏–Ω–≥–∞
            )

            logger.info(f"‚úÖ –ü–∞—Ä—Å–∏–Ω–≥ –∑–∞–≤–µ—Ä—à–µ–Ω: {result['message']}")
            return result

        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–∞—Ä—Å–∏–Ω–≥–µ –∞–Ω–æ–Ω—Å–æ–≤: {e}", exc_info=True)
            return {
                'changed': False,
                'new_snapshot': last_snapshot,
                'matched_content': None,
                'message': f"–û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞: {str(e)}"
            }

    def _strategy_any_change(
        self,
        soup: BeautifulSoup,
        html_content: str,
        last_snapshot: Optional[str] = None,
        **kwargs
    ) -> Dict[str, Any]:
        """
        –°—Ç—Ä–∞—Ç–µ–≥–∏—è 1: –û—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏–µ –ª—é–±—ã—Ö –∏–∑–º–µ–Ω–µ–Ω–∏–π –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ
        –°—Ä–∞–≤–Ω–∏–≤–∞–µ—Ç hash –≤—Å–µ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã
        """
        logger.info(f"üîç –°—Ç—Ä–∞—Ç–µ–≥–∏—è: –û—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏–µ –ª—é–±—ã—Ö –∏–∑–º–µ–Ω–µ–Ω–∏–π")

        # –£–¥–∞–ª—è–µ–º –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–∏–µ —ç–ª–µ–º–µ–Ω—Ç—ã –ø–µ—Ä–µ–¥ —Ö—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ–º
        # (–¥–∞—Ç—ã, –≤—Ä–µ–º–µ–Ω–∞, —Ç–æ–∫–µ–Ω—ã —Å–µ—Å—Å–∏–π –∏ —Ç.–¥.)
        clean_html = self._clean_html(html_content)

        # –°–æ–∑–¥–∞–µ–º hash —Å—Ç—Ä–∞–Ω–∏—Ü—ã
        page_hash = hashlib.md5(clean_html.encode('utf-8')).hexdigest()
        logger.debug(f"   –ù–æ–≤—ã–π hash: {page_hash}")

        # –ï—Å–ª–∏ –Ω–µ—Ç –ø—Ä–µ–¥—ã–¥—É—â–µ–≥–æ —Å–Ω–∏–º–∫–∞ - —ç—Ç–æ –ø–µ—Ä–≤–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞
        if not last_snapshot:
            logger.info(f"   –ü–µ—Ä–≤–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ - —Å–æ—Ö—Ä–∞–Ω—è–µ–º —Å–Ω–∏–º–æ–∫")
            return {
                'changed': False,
                'new_snapshot': page_hash,
                'matched_content': None,
                'message': '–ü–µ—Ä–≤–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ - —Å–Ω–∏–º–æ–∫ —Å–æ—Ö—Ä–∞–Ω–µ–Ω'
            }

        # –°—Ä–∞–≤–Ω–∏–≤–∞–µ–º —Å –ø—Ä–µ–¥—ã–¥—É—â–∏–º —Å–Ω–∏–º–∫–æ–º
        if page_hash != last_snapshot:
            logger.info(f"   ‚úÖ –û–±–Ω–∞—Ä—É–∂–µ–Ω—ã –∏–∑–º–µ–Ω–µ–Ω–∏—è!")
            logger.debug(f"   –°—Ç–∞—Ä—ã–π hash: {last_snapshot}")
            return {
                'changed': True,
                'new_snapshot': page_hash,
                'matched_content': f"–°—Ç—Ä–∞–Ω–∏—Ü–∞ –∏–∑–º–µ–Ω–∏–ª–∞—Å—å (hash: {page_hash[:8]}...)",
                'message': '–û–±–Ω–∞—Ä—É–∂–µ–Ω—ã –∏–∑–º–µ–Ω–µ–Ω–∏—è –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ'
            }
        else:
            logger.info(f"   –ò–∑–º–µ–Ω–µ–Ω–∏–π –Ω–µ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–æ")
            return {
                'changed': False,
                'new_snapshot': page_hash,
                'matched_content': None,
                'message': '–ò–∑–º–µ–Ω–µ–Ω–∏–π –Ω–µ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–æ'
            }

    def _strategy_element_change(
        self,
        soup: BeautifulSoup,
        html_content: str,
        last_snapshot: Optional[str] = None,
        css_selector: Optional[str] = None,
        **kwargs
    ) -> Dict[str, Any]:
        """
        –°—Ç—Ä–∞—Ç–µ–≥–∏—è 2: –û—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏–µ –∏–∑–º–µ–Ω–µ–Ω–∏–π –≤ –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–º —ç–ª–µ–º–µ–Ω—Ç–µ
        –°—Ä–∞–≤–Ω–∏–≤–∞–µ—Ç hash —Å–æ–¥–µ—Ä–∂–∏–º–æ–≥–æ —ç–ª–µ–º–µ–Ω—Ç–∞ –ø–æ CSS —Å–µ–ª–µ–∫—Ç–æ—Ä—É
        """
        logger.info(f"üéØ –°—Ç—Ä–∞—Ç–µ–≥–∏—è: –û—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏–µ –∏–∑–º–µ–Ω–µ–Ω–∏–π –≤ —ç–ª–µ–º–µ–Ω—Ç–µ")
        logger.info(f"   CSS —Å–µ–ª–µ–∫—Ç–æ—Ä: {css_selector}")

        if not css_selector:
            return {
                'changed': False,
                'new_snapshot': last_snapshot,
                'matched_content': None,
                'message': 'CSS —Å–µ–ª–µ–∫—Ç–æ—Ä –Ω–µ —É–∫–∞–∑–∞–Ω'
            }

        try:
            # –ù–∞—Ö–æ–¥–∏–º —ç–ª–µ–º–µ–Ω—Ç
            elements = soup.select(css_selector)

            if not elements:
                logger.warning(f"   ‚ö†Ô∏è –≠–ª–µ–º–µ–Ω—Ç –Ω–µ –Ω–∞–π–¥–µ–Ω: {css_selector}")
                return {
                    'changed': False,
                    'new_snapshot': last_snapshot,
                    'matched_content': None,
                    'message': f'–≠–ª–µ–º–µ–Ω—Ç –Ω–µ –Ω–∞–π–¥–µ–Ω: {css_selector}'
                }

            # –ë–µ—Ä–µ–º –ø–µ—Ä–≤—ã–π –Ω–∞–π–¥–µ–Ω–Ω—ã–π —ç–ª–µ–º–µ–Ω—Ç
            element = elements[0]
            element_content = element.get_text(strip=True)
            logger.debug(f"   –ù–∞–π–¥–µ–Ω —ç–ª–µ–º–µ–Ω—Ç, –∫–æ–Ω—Ç–µ–Ω—Ç: {element_content[:100]}...")

            # –°–æ–∑–¥–∞–µ–º hash —Å–æ–¥–µ—Ä–∂–∏–º–æ–≥–æ
            element_hash = hashlib.md5(element_content.encode('utf-8')).hexdigest()
            logger.debug(f"   –ù–æ–≤—ã–π hash —ç–ª–µ–º–µ–Ω—Ç–∞: {element_hash}")

            # –ï—Å–ª–∏ –Ω–µ—Ç –ø—Ä–µ–¥—ã–¥—É—â–µ–≥–æ —Å–Ω–∏–º–∫–∞ - —ç—Ç–æ –ø–µ—Ä–≤–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞
            if not last_snapshot:
                logger.info(f"   –ü–µ—Ä–≤–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ - —Å–æ—Ö—Ä–∞–Ω—è–µ–º —Å–Ω–∏–º–æ–∫")
                return {
                    'changed': False,
                    'new_snapshot': element_hash,
                    'matched_content': None,
                    'message': '–ü–µ—Ä–≤–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ - —Å–Ω–∏–º–æ–∫ —ç–ª–µ–º–µ–Ω—Ç–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω'
                }

            # –°—Ä–∞–≤–Ω–∏–≤–∞–µ–º —Å –ø—Ä–µ–¥—ã–¥—É—â–∏–º —Å–Ω–∏–º–∫–æ–º
            if element_hash != last_snapshot:
                logger.info(f"   ‚úÖ –û–±–Ω–∞—Ä—É–∂–µ–Ω—ã –∏–∑–º–µ–Ω–µ–Ω–∏—è –≤ —ç–ª–µ–º–µ–Ω—Ç–µ!")
                logger.debug(f"   –°—Ç–∞—Ä—ã–π hash: {last_snapshot}")
                return {
                    'changed': True,
                    'new_snapshot': element_hash,
                    'matched_content': element_content[:500],  # –ü–µ—Ä–≤—ã–µ 500 —Å–∏–º–≤–æ–ª–æ–≤
                    'message': f'–≠–ª–µ–º–µ–Ω—Ç –∏–∑–º–µ–Ω–∏–ª—Å—è: {css_selector}'
                }
            else:
                logger.info(f"   –ò–∑–º–µ–Ω–µ–Ω–∏–π –≤ —ç–ª–µ–º–µ–Ω—Ç–µ –Ω–µ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–æ")
                return {
                    'changed': False,
                    'new_snapshot': element_hash,
                    'matched_content': None,
                    'message': '–ò–∑–º–µ–Ω–µ–Ω–∏–π –≤ —ç–ª–µ–º–µ–Ω—Ç–µ –Ω–µ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–æ'
                }

        except Exception as e:
            logger.error(f"   ‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–∏—Å–∫–µ —ç–ª–µ–º–µ–Ω—Ç–∞: {e}")
            return {
                'changed': False,
                'new_snapshot': last_snapshot,
                'matched_content': None,
                'message': f'–û—à–∏–±–∫–∞ –ø–æ–∏—Å–∫–∞ —ç–ª–µ–º–µ–Ω—Ç–∞: {str(e)}'
            }

    def _strategy_any_keyword(
        self,
        soup: BeautifulSoup,
        html_content: str,
        keywords: Optional[List[str]] = None,
        use_browser: bool = False,
        **kwargs
    ) -> Dict[str, Any]:
        """
        –°—Ç—Ä–∞—Ç–µ–≥–∏—è 3: –ü–æ–∏—Å–∫ –ª—é–±–æ–≥–æ –∏–∑ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤
        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç True, –µ—Å–ª–∏ –Ω–∞–π–¥–µ–Ω–æ —Ö–æ—Ç—è –±—ã –æ–¥–Ω–æ –∫–ª—é—á–µ–≤–æ–µ —Å–ª–æ–≤–æ
        """
        logger.info(f"üìù –°—Ç—Ä–∞—Ç–µ–≥–∏—è: –ü–æ–∏—Å–∫ –ª—é–±–æ–≥–æ –∫–ª—é—á–µ–≤–æ–≥–æ —Å–ª–æ–≤–∞")
        logger.info(f"   –ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞: {keywords}")

        if not keywords:
            return {
                'changed': False,
                'new_snapshot': None,
                'matched_content': None,
                'message': '–ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –Ω–µ —É–∫–∞–∑–∞–Ω—ã'
            }

        # –ü–æ–ª—É—á–∞–µ–º —Ç–µ–∫—Å—Ç –≤—Å–µ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã
        page_text = soup.get_text().lower()

        # –ò—â–µ–º —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è
        matched_keywords = []
        for keyword in keywords:
            if keyword.lower() in page_text:
                matched_keywords.append(keyword)
                logger.debug(f"   ‚úÖ –ù–∞–π–¥–µ–Ω–æ: {keyword}")

        if matched_keywords:
            logger.info(f"   ‚úÖ –ù–∞–π–¥–µ–Ω–æ {len(matched_keywords)} –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤")
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º —Å—Å—ã–ª–∫–∏ –Ω–∞ –∞–Ω–æ–Ω—Å—ã, —Å–æ–¥–µ—Ä–∂–∞—â–∏–µ –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞
            announcement_links = self._extract_announcement_links(soup, keywords)
            
            result = {
                'changed': True,
                'new_snapshot': None,  # –î–ª—è keyword —Å—Ç—Ä–∞—Ç–µ–≥–∏–π —Å–Ω–∏–º–æ–∫ –Ω–µ –Ω—É–∂–µ–Ω
                'matched_content': f"–ù–∞–π–¥–µ–Ω—ã –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞: {', '.join(matched_keywords)}",
                'message': f'–ù–∞–π–¥–µ–Ω—ã –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞: {", ".join(matched_keywords)}'
            }
            
            # –î–æ–±–∞–≤–ª—è–µ–º –Ω–∞–π–¥–µ–Ω–Ω—ã–µ —Å—Å—ã–ª–∫–∏, –µ—Å–ª–∏ –æ–Ω–∏ –µ—Å—Ç—å
            if announcement_links:
                result['announcement_links'] = announcement_links
                logger.info(f"   üîó –ù–∞–π–¥–µ–Ω–æ {len(announcement_links)} —Å—Å—ã–ª–æ–∫ –Ω–∞ –∞–Ω–æ–Ω—Å—ã")
            else:
                logger.warning(f"   ‚ö†Ô∏è –ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –Ω–∞–π–¥–µ–Ω—ã, –Ω–æ —Å—Å—ã–ª–∫–∏ –Ω–∞ –∞–Ω–æ–Ω—Å—ã –Ω–µ –∏–∑–≤–ª–µ—á–µ–Ω—ã")
                logger.warning(f"   üí° –í–æ–∑–º–æ–∂–Ω–æ, —Å—Ç—Ä–∞–Ω–∏—Ü–∞ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–∏–π –∫–æ–Ω—Ç–µ–Ω—Ç –∏–ª–∏ –Ω–µ—Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—É—é —Å—Ç—Ä—É–∫—Ç—É—Ä—É")
                # –î–æ–±–∞–≤–ª—è–µ–º –æ—Ç–ª–∞–¥–æ—á–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é
                total_links = len(soup.find_all('a', href=True))
                logger.warning(f"   üìä –í—Å–µ–≥–æ —Å—Å—ã–ª–æ–∫ –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ: {total_links}")
                logger.warning(f"   üåê –ë—Ä–∞—É–∑–µ—Ä–Ω—ã–π –ø–∞—Ä—Å–∏–Ω–≥: {'–≤–∫–ª—é—á–µ–Ω' if use_browser else '–í–´–ö–õ–Æ–ß–ï–ù'}")
                
                # –î–æ–±–∞–≤–ª—è–µ–º –¥–∏–∞–≥–Ω–æ—Å—Ç–∏—á–µ—Å–∫—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –≤ —Ä–µ–∑—É–ª—å—Ç–∞—Ç
                result['debug_info'] = {
                    'total_links_on_page': total_links,
                    'browser_parsing_enabled': use_browser,
                    'page_size': len(html_content) if html_content else 0
                }
            
            return result
        else:
            logger.info(f"   –ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã")
            return {
                'changed': False,
                'new_snapshot': None,
                'matched_content': None,
                'message': '–ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã'
            }

    def _strategy_all_keywords(
        self,
        soup: BeautifulSoup,
        html_content: str,
        keywords: Optional[List[str]] = None,
        **kwargs
    ) -> Dict[str, Any]:
        """
        –°—Ç—Ä–∞—Ç–µ–≥–∏—è 4: –í—Å–µ –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –¥–æ–ª–∂–Ω—ã –ø—Ä–∏—Å—É—Ç—Å—Ç–≤–æ–≤–∞—Ç—å
        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç True —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ –Ω–∞–π–¥–µ–Ω—ã –í–°–ï –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞
        """
        logger.info(f"üìö –°—Ç—Ä–∞—Ç–µ–≥–∏—è: –ü–æ–∏—Å–∫ –≤—Å–µ—Ö –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤")
        logger.info(f"   –ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞: {keywords}")

        if not keywords:
            return {
                'changed': False,
                'new_snapshot': None,
                'matched_content': None,
                'message': '–ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –Ω–µ —É–∫–∞–∑–∞–Ω—ã'
            }

        # –ü–æ–ª—É—á–∞–µ–º —Ç–µ–∫—Å—Ç –≤—Å–µ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã
        page_text = soup.get_text().lower()

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –≤—Å–µ—Ö –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤
        matched_keywords = []
        missing_keywords = []

        for keyword in keywords:
            if keyword.lower() in page_text:
                matched_keywords.append(keyword)
                logger.debug(f"   ‚úÖ –ù–∞–π–¥–µ–Ω–æ: {keyword}")
            else:
                missing_keywords.append(keyword)
                logger.debug(f"   ‚ùå –ù–µ –Ω–∞–π–¥–µ–Ω–æ: {keyword}")

        if len(matched_keywords) == len(keywords):
            logger.info(f"   ‚úÖ –í—Å–µ –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –Ω–∞–π–¥–µ–Ω—ã!")
            return {
                'changed': True,
                'new_snapshot': None,
                'matched_content': f"–í—Å–µ –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –Ω–∞–π–¥–µ–Ω—ã: {', '.join(matched_keywords)}",
                'message': f'–í—Å–µ –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –Ω–∞–π–¥–µ–Ω—ã: {", ".join(matched_keywords)}'
            }
        else:
            logger.info(f"   –ù–∞–π–¥–µ–Ω–æ {len(matched_keywords)}/{len(keywords)} –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤")
            return {
                'changed': False,
                'new_snapshot': None,
                'matched_content': None,
                'message': f'–ù–µ –Ω–∞–π–¥–µ–Ω—ã: {", ".join(missing_keywords)}'
            }

    def _strategy_regex(
        self,
        soup: BeautifulSoup,
        html_content: str,
        regex_pattern: Optional[str] = None,
        **kwargs
    ) -> Dict[str, Any]:
        """
        –°—Ç—Ä–∞—Ç–µ–≥–∏—è 5: –ü–æ–∏—Å–∫ –ø–æ —Ä–µ–≥—É–ª—è—Ä–Ω–æ–º—É –≤—ã—Ä–∞–∂–µ–Ω–∏—é
        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç True, –µ—Å–ª–∏ –Ω–∞–π–¥–µ–Ω–æ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ —Å regex
        """
        logger.info(f"‚ö° –°—Ç—Ä–∞—Ç–µ–≥–∏—è: –ü–æ–∏—Å–∫ –ø–æ —Ä–µ–≥—É–ª—è—Ä–Ω–æ–º—É –≤—ã—Ä–∞–∂–µ–Ω–∏—é")
        logger.info(f"   Regex: {regex_pattern}")

        if not regex_pattern:
            return {
                'changed': False,
                'new_snapshot': None,
                'matched_content': None,
                'message': '–†–µ–≥—É–ª—è—Ä–Ω–æ–µ –≤—ã—Ä–∞–∂–µ–Ω–∏–µ –Ω–µ —É–∫–∞–∑–∞–Ω–æ'
            }

        try:
            # –ö–æ–º–ø–∏–ª–∏—Ä—É–µ–º regex
            pattern = re.compile(regex_pattern, re.IGNORECASE)

            # –ü–æ–ª—É—á–∞–µ–º —Ç–µ–∫—Å—Ç —Å—Ç—Ä–∞–Ω–∏—Ü—ã
            page_text = soup.get_text()

            # –ò—â–µ–º —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è
            matches = pattern.findall(page_text)

            if matches:
                logger.info(f"   ‚úÖ –ù–∞–π–¥–µ–Ω–æ {len(matches)} —Å–æ–≤–ø–∞–¥–µ–Ω–∏–π")
                # –ë–µ—Ä–µ–º –ø–µ—Ä–≤—ã–µ 5 —Å–æ–≤–ø–∞–¥–µ–Ω–∏–π
                sample_matches = matches[:5]
                return {
                    'changed': True,
                    'new_snapshot': None,
                    'matched_content': f"–ù–∞–π–¥–µ–Ω–æ {len(matches)} —Å–æ–≤–ø–∞–¥–µ–Ω–∏–π: {', '.join(sample_matches)}",
                    'message': f'–ù–∞–π–¥–µ–Ω–æ {len(matches)} —Å–æ–≤–ø–∞–¥–µ–Ω–∏–π —Å regex'
                }
            else:
                logger.info(f"   –°–æ–≤–ø–∞–¥–µ–Ω–∏–π –Ω–µ –Ω–∞–π–¥–µ–Ω–æ")
                return {
                    'changed': False,
                    'new_snapshot': None,
                    'matched_content': None,
                    'message': '–°–æ–≤–ø–∞–¥–µ–Ω–∏–π —Å regex –Ω–µ –Ω–∞–π–¥–µ–Ω–æ'
                }

        except re.error as e:
            logger.error(f"   ‚ùå –û—à–∏–±–∫–∞ –≤ —Ä–µ–≥—É–ª—è—Ä–Ω–æ–º –≤—ã—Ä–∞–∂–µ–Ω–∏–∏: {e}")
            return {
                'changed': False,
                'new_snapshot': None,
                'matched_content': None,
                'message': f'–û—à–∏–±–∫–∞ –≤ regex: {str(e)}'
            }

    def _fetch_with_browser(self) -> Optional[str]:
        """
        –ó–∞–≥—Ä—É–∂–∞–µ—Ç —Å—Ç—Ä–∞–Ω–∏—Ü—É —á–µ—Ä–µ–∑ Playwright (–±—Ä–∞—É–∑–µ—Ä–Ω—ã–π –ø–∞—Ä—Å–∏–Ω–≥) –¥–ª—è –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–æ–≥–æ –∫–æ–Ω—Ç–µ–Ω—Ç–∞
        –° –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–º fallback: —Å–Ω–∞—á–∞–ª–∞ –ë–ï–ó –ø—Ä–æ–∫—Å–∏ (–±—ã—Å—Ç—Ä–µ–µ), –ø–æ—Ç–æ–º —Å –ø—Ä–æ–∫—Å–∏
        """
        from playwright.sync_api import sync_playwright, TimeoutError as PlaywrightTimeout
        
        logger.info("üöÄ –ó–∞–ø—É—Å–∫ Playwright –¥–ª—è –±—Ä–∞—É–∑–µ—Ä–Ω–æ–≥–æ –ø–∞—Ä—Å–∏–Ω–≥–∞...")
        
        # –ü–æ–ª—É—á–∞–µ–º –ø—Ä–æ–∫—Å–∏ –∏ User-Agent
        proxy, user_agent = self.rotation_manager.get_optimal_combination(self._extract_exchange_from_url(self.url))
        
        # –ü–æ–ø—ã—Ç–∫–∞ 1: –ë–ï–ó –ü–†–û–ö–°–ò (–æ–±—ã—á–Ω–æ –±—ã—Å—Ç—Ä–µ–µ –∏ –Ω–∞–¥–µ–∂–Ω–µ–µ –¥–ª—è MEXC)
        logger.info("üîß –ü–æ–ø—ã—Ç–∫–∞ 1: –ó–∞–≥—Ä—É–∑–∫–∞ –ë–ï–ó –ü–†–û–ö–°–ò (–±—ã—Å—Ç—Ä–µ–µ)")
        result = self._try_load_with_playwright(None, user_agent)
        if result:
            return result
        
        # –ü–æ–ø—ã—Ç–∫–∞ 2: –° –ø—Ä–æ–∫—Å–∏ (–µ—Å–ª–∏ –±–µ–∑ –ø—Ä–æ–∫—Å–∏ –Ω–µ –ø–æ–ª—É—á–∏–ª–æ—Å—å)
        if proxy:
            logger.warning("‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –±–µ–∑ –ø—Ä–æ–∫—Å–∏, –ø—Ä–æ–±—É–µ–º –° –ü–†–û–ö–°–ò...")
            logger.info(f"üîß –ü–æ–ø—ã—Ç–∫–∞ 2: –ó–∞–≥—Ä—É–∑–∫–∞ –° –ü–†–û–ö–°–ò {proxy.address}")
            result = self._try_load_with_playwright(proxy, user_agent)
            if result:
                return result
        
        logger.error("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å —Å—Ç—Ä–∞–Ω–∏—Ü—É –Ω–∏ –±–µ–∑ –ø—Ä–æ–∫—Å–∏, –Ω–∏ —Å –ø—Ä–æ–∫—Å–∏")
        return None
    
    def _try_load_with_playwright(self, proxy, user_agent) -> Optional[str]:
        """
        –ü–æ–ø—ã—Ç–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ —Å—Ç—Ä–∞–Ω–∏—Ü—ã —á–µ—Ä–µ–∑ Playwright —Å –∑–∞–¥–∞–Ω–Ω—ã–º–∏ –Ω–∞—Å—Ç—Ä–æ–π–∫–∞–º–∏
        """
        try:
            from playwright.sync_api import sync_playwright, TimeoutError as PlaywrightTimeout
            
            with sync_playwright() as p:
                # –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –±—Ä–∞—É–∑–µ—Ä–∞
                browser_args = [
                    '--disable-blink-features=AutomationControlled',
                    '--no-sandbox',
                    '--disable-setuid-sandbox',
                    '--disable-dev-shm-usage',  # –î–ª—è —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏
                ]
                
                launch_options = {
                    'headless': True,
                    'args': browser_args
                }
                
                # –î–æ–±–∞–≤–ª—è–µ–º –ø—Ä–æ–∫—Å–∏ —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ —É–∫–∞–∑–∞–Ω
                if proxy:
                    launch_options['proxy'] = {
                        'server': f"{proxy.protocol}://{proxy.address}"
                    }
                
                browser = p.chromium.launch(**launch_options)
                
                # –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
                context_options = {
                    'viewport': {'width': 1920, 'height': 1080},
                    'locale': 'en-US',
                    'ignore_https_errors': True,  # –ò–≥–Ω–æ—Ä–∏—Ä—É–µ–º SSL –æ—à–∏–±–∫–∏
                }
                
                if user_agent:
                    context_options['user_agent'] = user_agent.user_agent_string
                
                context = browser.new_context(**context_options)
                page = context.new_page()
                
                # –ü—Ä–∏–º–µ–Ω—è–µ–º stealth (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
                try:
                    from playwright_stealth import Stealth
                    stealth_obj = Stealth()
                    stealth_obj.apply_stealth_sync(page)
                except:
                    pass  # –ò–≥–Ω–æ—Ä–∏—Ä—É–µ–º –æ—à–∏–±–∫–∏ stealth
                
                logger.info(f"üåê –ó–∞–≥—Ä—É–∑–∫–∞ —Å—Ç—Ä–∞–Ω–∏—Ü—ã: {self.url}")
                
                # –ü–µ—Ä–µ—Ö–æ–¥–∏–º –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü—É - –∏—Å–ø–æ–ª—å–∑—É–µ–º networkidle –¥–ª—è MEXC (–∂–¥–µ—Ç –∫–æ–≥–¥–∞ —Å–µ—Ç—å —É—Å–ø–æ–∫–æ–∏—Ç—Å—è)
                # –¢–∞–π–º–∞—É—Ç 60 —Å–µ–∫—É–Ω–¥ - –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–ª—è –±–æ–ª—å—à–∏–Ω—Å—Ç–≤–∞ —Å–ª—É—á–∞–µ–≤
                try:
                    page.goto(self.url, wait_until='networkidle', timeout=60000)
                    logger.info("‚úÖ –°—Ç—Ä–∞–Ω–∏—Ü–∞ –ø–æ–ª–Ω–æ—Å—Ç—å—é –∑–∞–≥—Ä—É–∂–µ–Ω–∞ (networkidle)")
                except PlaywrightTimeout:
                    logger.info("‚è±Ô∏è Networkidle —Ç–∞–π–º–∞—É—Ç, –Ω–æ –∫–æ–Ω—Ç–µ–Ω—Ç —É–∂–µ –∑–∞–≥—Ä—É–∂–µ–Ω")
                    # –î–∞–∂–µ –µ—Å–ª–∏ —Ç–∞–π–º–∞—É—Ç, –∫–æ–Ω—Ç–µ–Ω—Ç —É–∂–µ –∑–∞–≥—Ä—É–∑–∏–ª—Å—è
                
                # –ú–∏–Ω–∏–º–∞–ª—å–Ω–æ–µ –æ–∂–∏–¥–∞–Ω–∏–µ –¥–ª—è —Ä–µ–Ω–¥–µ—Ä–∏–Ω–≥–∞ React
                page.wait_for_timeout(2000)
                
                # –°–∫—Ä–æ–ª–ª–∏–º –¥–ª—è —Ç—Ä–∏–≥–≥–µ—Ä–∞ lazy loading
                try:
                    page.evaluate("window.scrollTo(0, document.body.scrollHeight)")
                    page.wait_for_timeout(1000)
                except:
                    pass
                
                # –ü–æ–ª—É—á–∞–µ–º HTML
                html_content = page.content()
                
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ –ø–æ–ª—É—á–∏–ª–∏ —Ä–µ–∞–ª—å–Ω—ã–π –∫–æ–Ω—Ç–µ–Ω—Ç (–Ω–µ –ø—É—Å—Ç—É—é —Å—Ç—Ä–∞–Ω–∏—Ü—É)
                if len(html_content) < 1000:
                    logger.warning(f"‚ö†Ô∏è –ü–æ–¥–æ–∑—Ä–∏—Ç–µ–ª—å–Ω–æ –º–∞–ª–µ–Ω—å–∫–∏–π —Ä–∞–∑–º–µ—Ä —Å—Ç—Ä–∞–Ω–∏—Ü—ã: {len(html_content)} –±–∞–π—Ç")
                    context.close()
                    browser.close()
                    return None
                
                logger.info(f"‚úÖ –°—Ç—Ä–∞–Ω–∏—Ü–∞ –∑–∞–≥—Ä—É–∂–µ–Ω–∞ ({len(html_content)} –±–∞–π—Ç)")
                
                # –ó–∞–∫—Ä—ã–≤–∞–µ–º –±—Ä–∞—É–∑–µ—Ä
                context.close()
                browser.close()
                
                return html_content
                
        except PlaywrightTimeout:
            logger.warning("‚è∞ –¢–∞–π–º–∞—É—Ç –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã")
            return None
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ: {e}")
            return None

    def _extract_announcement_links(self, soup: BeautifulSoup, keywords: List[str]) -> List[Dict[str, str]]:
        """
        –ò–∑–≤–ª–µ–∫–∞–µ—Ç —Å—Å—ã–ª–∫–∏ –Ω–∞ –∞–Ω–æ–Ω—Å—ã, –∫–æ—Ç–æ—Ä—ã–µ —Å–æ–¥–µ—Ä–∂–∞—Ç —É–∫–∞–∑–∞–Ω–Ω—ã–µ –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞
        
        Args:
            soup: –û–±—ä–µ–∫—Ç BeautifulSoup –¥–ª—è –ø–∞—Ä—Å–∏–Ω–≥–∞
            keywords: –°–ø–∏—Å–æ–∫ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤ –¥–ª—è –ø–æ–∏—Å–∫–∞
            
        Returns:
            –°–ø–∏—Å–æ–∫ —Å–ª–æ–≤–∞—Ä–µ–π —Å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π –æ –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö –∞–Ω–æ–Ω—Å–∞—Ö:
            [{'title': '–ó–∞–≥–æ–ª–æ–≤–æ–∫', 'url': 'https://...', 'matched_keywords': ['keyword1'], 'description': '...'}]
        """
        announcement_links = []
        
        # –ü–∞—Ç—Ç–µ—Ä–Ω—ã –¥–ª—è —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –±–∏—Ä–∂
        link_patterns = {
            'mexc.co': {
                # MEXC –∏—Å–ø–æ–ª—å–∑—É–µ—Ç React –∏ –¥–∏–Ω–∞–º–∏—á–µ—Å–∫—É—é –∑–∞–≥—Ä—É–∑–∫—É
                # –ò—â–µ–º –≤—Å–µ —Å—Å—ã–ª–∫–∏ –Ω–∞ /announcements/article/
                'selectors': [
                    'a[href*="/announcements/article/"]',
                    'a[href*="/support/articles/"]',
                ],
                'container_selectors': ['article', 'div[class*="article"]', 'div[class*="announcement"]', 'div[class*="news"]'],
                'url_prefix': 'https://www.mexc.co'
            },
            'binance.com': {
                'selectors': ['a[href*="/support/announcement/"]', '.css-article a'],
                'container_selectors': ['article', '.article-item'],
                'url_prefix': 'https://www.binance.com'
            },
            'bybit.com': {
                'selectors': ['a[href*="/announcements/"]', '.announcement-list a'],
                'container_selectors': ['.announcement-item'],
                'url_prefix': 'https://www.bybit.com'
            },
            'okx.com': {
                'selectors': ['a[href*="/support/hc/"]', '.article-item a'],
                'container_selectors': ['.article-card'],
                'url_prefix': 'https://www.okx.com'
            },
            'gate.io': {
                'selectors': ['a[href*="/article/"]', '.article-link'],
                'container_selectors': ['.article-wrapper'],
                'url_prefix': 'https://www.gate.io'
            },
            'kucoin.com': {
                'selectors': ['a[href*="/news/"]', '.news-list a'],
                'container_selectors': ['.news-card'],
                'url_prefix': 'https://www.kucoin.com'
            }
        }
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –±–∏—Ä–∂—É –∏–∑ URL
        exchange = None
        for exch in link_patterns.keys():
            if exch in self.url:
                exchange = exch
                break
        
        if not exchange:
            logger.debug("   ‚ö†Ô∏è –ù–µ–∏–∑–≤–µ—Å—Ç–Ω–∞—è –±–∏—Ä–∂–∞, –∏—Å–ø–æ–ª—å–∑—É–µ–º –æ–±—â–∏–π –ø–∞—Ç—Ç–µ—Ä–Ω")
            # –û–±—â–∏–π –ø–∞—Ç—Ç–µ—Ä–Ω –¥–ª—è –ª—é–±—ã—Ö —Å—Å—ã–ª–æ–∫
            all_links = soup.find_all('a', href=True)
            logger.info(f"   üìä –ù–∞–π–¥–µ–Ω–æ {len(all_links)} —Å—Å—ã–ª–æ–∫ –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ")
        else:
            # –ò—â–µ–º —Å—Å—ã–ª–∫–∏ –ø–æ —Å–ø–µ—Ü–∏—Ñ–∏—á–Ω—ã–º —Å–µ–ª–µ–∫—Ç–æ—Ä–∞–º
            config = link_patterns[exchange]
            all_links = []
            for selector in config['selectors']:
                found = soup.select(selector)
                all_links.extend(found)
                logger.debug(f"   üîç –°–µ–ª–µ–∫—Ç–æ—Ä '{selector}': –Ω–∞–π–¥–µ–Ω–æ {len(found)} —Å—Å—ã–ª–æ–∫")
            
            logger.info(f"   üìä –ù–∞–π–¥–µ–Ω–æ {len(all_links)} —Å—Å—ã–ª–æ–∫ –∞–Ω–æ–Ω—Å–æ–≤ –¥–ª—è {exchange}")
        
        # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –Ω–∞–π–¥–µ–Ω–Ω—ã–µ —Å—Å—ã–ª–∫–∏
        seen_urls = set()  # –ò–∑–±–µ–≥–∞–µ–º –¥—É–±–ª–∏–∫–∞—Ç–æ–≤
        
        for link in all_links:
            try:
                href = link.get('href', '')
                if not href:
                    continue
                
                # –§–æ—Ä–º–∏—Ä—É–µ–º –ø–æ–ª–Ω—ã–π URL
                if href.startswith('http'):
                    full_url = href
                elif href.startswith('/'):
                    # –û—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω–∞—è —Å—Å—ã–ª–∫–∞
                    if exchange and exchange in link_patterns:
                        full_url = link_patterns[exchange]['url_prefix'] + href
                    else:
                        # –ò–∑–≤–ª–µ–∫–∞–µ–º –±–∞–∑–æ–≤—ã–π URL –∏–∑ self.url
                        from urllib.parse import urlparse
                        parsed = urlparse(self.url)
                        full_url = f"{parsed.scheme}://{parsed.netloc}{href}"
                else:
                    continue
                
                # –ò–∑–±–µ–≥–∞–µ–º –¥—É–±–ª–∏–∫–∞—Ç–æ–≤
                if full_url in seen_urls:
                    continue
                
                # –ü–æ–ª—É—á–∞–µ–º —Ç–µ–∫—Å—Ç —Å—Å—ã–ª–∫–∏
                link_text = link.get_text(strip=True)
                
                # –ò—â–µ–º —Ä–æ–¥–∏—Ç–µ–ª—å—Å–∫–∏–π –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä –¥–ª—è –±–æ–ª–µ–µ –ø–æ–ª–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
                parent_text = ""
                description = ""
                
                if exchange and exchange in link_patterns:
                    # –ò—â–µ–º —Ä–æ–¥–∏—Ç–µ–ª—å—Å–∫–∏–π –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä –∞–Ω–æ–Ω—Å–∞
                    parent = None
                    for container_sel in link_patterns[exchange].get('container_selectors', []):
                        parent = link.find_parent(container_sel) if not container_sel.startswith('.') else link.find_parent(class_=container_sel.replace('.', ''))
                        if parent:
                            break
                    
                    if not parent:
                        # –ü—Ä–æ—Å—Ç–æ –±–µ—Ä–µ–º –±–ª–∏–∂–∞–π—à–µ–≥–æ —Ä–æ–¥–∏—Ç–µ–ª—è (div, article, li)
                        parent = link.find_parent(['div', 'article', 'li', 'section'])
                    
                    if parent:
                        parent_text = parent.get_text(separator=' ', strip=True)
                        # –ò—â–µ–º –æ–ø–∏—Å–∞–Ω–∏–µ (–æ–±—ã—á–Ω–æ –≤ <p>, <span>, –∏–ª–∏ <div> –≤–Ω—É—Ç—Ä–∏ –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä–∞)
                        desc_elem = parent.find(['p', 'span', 'div'], recursive=True)
                        if desc_elem and desc_elem != link:
                            description = desc_elem.get_text(strip=True)[:300]  # –ü–µ—Ä–≤—ã–µ 300 —Å–∏–º–≤–æ–ª–æ–≤
                else:
                    # –î–ª—è –Ω–µ–∏–∑–≤–µ—Å—Ç–Ω—ã—Ö –±–∏—Ä–∂ –±–µ—Ä–µ–º –±–ª–∏–∂–∞–π—à–µ–≥–æ —Ä–æ–¥–∏—Ç–µ–ª—è
                    parent = link.find_parent(['div', 'article', 'li'])
                    if parent:
                        parent_text = parent.get_text(separator=' ', strip=True)
                
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –≤ —Ç–µ–∫—Å—Ç–µ —Å—Å—ã–ª–∫–∏ –∏ —Ä–æ–¥–∏—Ç–µ–ª—å—Å–∫–æ–º –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä–µ
                search_text = (link_text + ' ' + parent_text).lower()
                matched = []
                for keyword in keywords:
                    if keyword.lower() in search_text:
                        matched.append(keyword)
                
                # –ï—Å–ª–∏ –Ω–∞—à–ª–∏ —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è, –¥–æ–±–∞–≤–ª—è–µ–º —Å—Å—ã–ª–∫—É
                if matched:
                    announcement_data = {
                        'title': link_text[:200] if link_text else '–ë–µ–∑ –Ω–∞–∑–≤–∞–Ω–∏—è',  # –£–≤–µ–ª–∏—á–∏–ª–∏ –ª–∏–º–∏—Ç
                        'url': full_url,
                        'matched_keywords': matched
                    }
                    
                    # –î–æ–±–∞–≤–ª—è–µ–º –æ–ø–∏—Å–∞–Ω–∏–µ, –µ—Å–ª–∏ –æ–Ω–æ –µ—Å—Ç—å
                    if description and description != link_text:
                        announcement_data['description'] = description
                    
                    announcement_links.append(announcement_data)
                    seen_urls.add(full_url)
                    
                    logger.info(f"   ‚úÖ –ù–∞–π–¥–µ–Ω –∞–Ω–æ–Ω—Å: {link_text[:80]}...")
                    logger.debug(f"      URL: {full_url}")
                    logger.debug(f"      –ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞: {', '.join(matched)}")
                    if description:
                        logger.debug(f"      –û–ø–∏—Å–∞–Ω–∏–µ: {description[:100]}...")
            
            except Exception as e:
                logger.debug(f"   ‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ —Å—Å—ã–ª–∫–∏: {e}")
                continue
        
        # FALLBACK: –ï—Å–ª–∏ –Ω–µ –Ω–∞—à–ª–∏ –Ω–∏ –æ–¥–Ω–æ–π —Å—Å—ã–ª–∫–∏ —á–µ—Ä–µ–∑ —Å–µ–ª–µ–∫—Ç–æ—Ä—ã,
        # –ø–æ–ø—Ä–æ–±—É–µ–º –Ω–∞–π—Ç–∏ –í–°–ï —Å—Å—ã–ª–∫–∏ –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ –∏ –ø—Ä–æ–≤–µ—Ä–∏—Ç—å –∏—Ö –∫–æ–Ω—Ç–µ–∫—Å—Ç
        if len(announcement_links) == 0:
            logger.warning(f"   ‚ö†Ô∏è –ù–µ –Ω–∞–π–¥–µ–Ω–æ —Å—Å—ã–ª–æ–∫ —á–µ—Ä–µ–∑ —Å–µ–ª–µ–∫—Ç–æ—Ä—ã, –ø—Ä–æ–±—É–µ–º fallback...")
            fallback_links = self._extract_links_fallback(soup, keywords, link_patterns.get(exchange, {}))
            announcement_links.extend(fallback_links)
        
        # SUPER FALLBACK: –ï—Å–ª–∏ –∏ fallback –Ω–µ –ø–æ–º–æ–≥, –ø—Ä–æ–±—É–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–π —Å–ø–æ—Å–æ–±
        if len(announcement_links) == 0:
            logger.warning(f"   ‚ö†Ô∏è Fallback –Ω–µ –ø–æ–º–æ–≥, –ø—Ä–æ–±—É–µ–º SUPER FALLBACK (–ø–æ–∏—Å–∫ –ø–æ –≤—Å–µ–º —Å—Å—ã–ª–∫–∞–º)...")
            super_fallback_links = self._extract_links_super_fallback(soup, keywords, link_patterns.get(exchange, {}))
            announcement_links.extend(super_fallback_links)
        
        # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å—Å—ã–ª–æ–∫ (—Ç–æ–ø-10)
        return announcement_links[:10]
    
    def _extract_links_fallback(self, soup: BeautifulSoup, keywords: List[str], exchange_config: dict) -> List[Dict[str, str]]:
        """
        Fallback –º–µ—Ç–æ–¥ –¥–ª—è –ø–æ–∏—Å–∫–∞ —Å—Å—ã–ª–æ–∫, –∫–æ–≥–¥–∞ –æ—Å–Ω–æ–≤–Ω—ã–µ —Å–µ–ª–µ–∫—Ç–æ—Ä—ã –Ω–µ –¥–∞—é—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞.
        –ò—â–µ—Ç –í–°–ï —Ç–µ–∫—Å—Ç–æ–≤—ã–µ –±–ª–æ–∫–∏ —Å –∫–ª—é—á–µ–≤—ã–º–∏ —Å–ª–æ–≤–∞–º–∏ –∏ –ø—ã—Ç–∞–µ—Ç—Å—è –Ω–∞–π—Ç–∏ —Ä—è–¥–æ–º —Å—Å—ã–ª–∫–∏.
        """
        logger.info("   üîÑ FALLBACK: –ü–æ–∏—Å–∫ —Å—Å—ã–ª–æ–∫ –ø–æ –≤—Å–µ–π —Å—Ç—Ä–∞–Ω–∏—Ü–µ...")
        announcement_links = []
        seen_urls = set()
        
        # –ò—â–µ–º –≤—Å–µ —ç–ª–µ–º–µ–Ω—Ç—ã, —Å–æ–¥–µ—Ä–∂–∞—â–∏–µ –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞
        for keyword in keywords:
            # –ò—â–µ–º –≤—Å–µ —ç–ª–µ–º–µ–Ω—Ç—ã —Å —Ç–µ–∫—Å—Ç–æ–º, —Å–æ–¥–µ—Ä–∂–∞—â–∏–º –∫–ª—é—á–µ–≤–æ–µ —Å–ª–æ–≤–æ
            elements = soup.find_all(string=lambda text: text and keyword.lower() in text.lower())
            
            logger.debug(f"   üîç –ù–∞–π–¥–µ–Ω–æ {len(elements)} —ç–ª–µ–º–µ–Ω—Ç–æ–≤ —Å –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–æ–º '{keyword}'")
            
            for element in elements:
                try:
                    # –ò—â–µ–º –±–ª–∏–∂–∞–π—à—É—é —Å—Å—ã–ª–∫—É (–≤ —Ä–æ–¥–∏—Ç–µ–ª—è—Ö, —Å–æ—Å–µ–¥—è—Ö –∏–ª–∏ –¥–µ—Ç—è—Ö)
                    parent = element.parent
                    link = None
                    container = None
                    
                    # –°—Ç—Ä–∞—Ç–µ–≥–∏—è 1: –ü—Ä–æ–≤–µ—Ä—è–µ–º, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ —Å–∞–º —ç–ª–µ–º–µ–Ω—Ç –∏–ª–∏ –µ–≥–æ —Ä–æ–¥–∏—Ç–µ–ª—å —Å—Å—ã–ª–∫–æ–π
                    current = parent
                    for _ in range(5):  # –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥–æ 5 —É—Ä–æ–≤–Ω–µ–π –≤–≤–µ—Ä—Ö
                        if current and current.name == 'a' and current.get('href'):
                            link = current
                            container = current.find_parent(['div', 'article', 'section', 'li'])
                            break
                        # –ò—â–µ–º —Å—Å—ã–ª–∫—É –≤–Ω—É—Ç—Ä–∏ —Ç–µ–∫—É—â–µ–≥–æ —É—Ä–æ–≤–Ω—è
                        if current:
                            link = current.find('a', href=True)
                            if link:
                                container = current
                                break
                            current = current.parent
                        else:
                            break
                    
                    # –°—Ç—Ä–∞—Ç–µ–≥–∏—è 2: –ï—Å–ª–∏ –Ω–µ –Ω–∞—à–ª–∏ –≤ —Ä–æ–¥–∏—Ç–µ–ª—è—Ö, –∏—â–µ–º –≤ —Å–æ—Å–µ–¥–Ω–∏—Ö —ç–ª–µ–º–µ–Ω—Ç–∞—Ö
                    if not link and parent:
                        # –ò—â–µ–º –±–ª–∏–∂–∞–π—à–∏–π —Ä–æ–¥–∏—Ç–µ–ª—å—Å–∫–∏–π –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä
                        container = parent.find_parent(['div', 'article', 'section', 'li', 'tr'])
                        if container:
                            # –ò—â–µ–º –í–°–ï —Å—Å—ã–ª–∫–∏ –≤–Ω—É—Ç—Ä–∏ –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä–∞
                            links_in_container = container.find_all('a', href=True)
                            if links_in_container:
                                # –ë–µ—Ä–µ–º –ø–µ—Ä–≤—É—é –ø–æ–¥—Ö–æ–¥—è—â—É—é —Å—Å—ã–ª–∫—É
                                link = links_in_container[0]
                                logger.debug(f"   üìç –ù–∞–π–¥–µ–Ω–∞ —Å—Å—ã–ª–∫–∞ –≤ —Å–æ—Å–µ–¥–Ω–µ–º —ç–ª–µ–º–µ–Ω—Ç–µ –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä–∞")
                    
                    # –°—Ç—Ä–∞—Ç–µ–≥–∏—è 3: –ï—Å–ª–∏ –≤—Å–µ –µ—â–µ –Ω–µ –Ω–∞—à–ª–∏, –∏—â–µ–º —Å–ª–µ–¥—É—é—â–∏–π/–ø—Ä–µ–¥—ã–¥—É—â–∏–π —ç–ª–µ–º–µ–Ω—Ç —Å —Å—Å—ã–ª–∫–æ–π
                    if not link and parent:
                        # –ò—â–µ–º —Å–ª–µ–¥—É—é—â–∏–π —ç–ª–µ–º–µ–Ω—Ç
                        next_sibling = parent.find_next_sibling()
                        if next_sibling:
                            link = next_sibling.find('a', href=True) if next_sibling.name != 'a' else next_sibling
                        
                        # –ï—Å–ª–∏ –Ω–µ –Ω–∞—à–ª–∏, –∏—â–µ–º –ø—Ä–µ–¥—ã–¥—É—â–∏–π —ç–ª–µ–º–µ–Ω—Ç
                        if not link:
                            prev_sibling = parent.find_previous_sibling()
                            if prev_sibling:
                                link = prev_sibling.find('a', href=True) if prev_sibling.name != 'a' else prev_sibling
                    
                    if link:
                        href = link.get('href', '')
                        if not href:
                            continue
                        
                        # –§–æ—Ä–º–∏—Ä—É–µ–º –ø–æ–ª–Ω—ã–π URL
                        if href.startswith('http'):
                            full_url = href
                        elif href.startswith('/'):
                            url_prefix = exchange_config.get('url_prefix', '')
                            if url_prefix:
                                full_url = url_prefix + href
                            else:
                                from urllib.parse import urlparse
                                parsed = urlparse(self.url)
                                full_url = f"{parsed.scheme}://{parsed.netloc}{href}"
                        else:
                            continue
                        
                        # –ò–∑–±–µ–≥–∞–µ–º –¥—É–±–ª–∏–∫–∞—Ç–æ–≤
                        if full_url in seen_urls:
                            continue
                        
                        # –ü–æ–ª—É—á–∞–µ–º —Ç–µ–∫—Å—Ç –∏ –æ–ø–∏—Å–∞–Ω–∏–µ
                        link_text = link.get_text(strip=True)
                        
                        # –ï—Å–ª–∏ —Ç–µ–∫—Å—Ç —Å—Å—ã–ª–∫–∏ –ø—É—Å—Ç–æ–π –∏–ª–∏ —Å–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–∏–π, –±–µ—Ä–µ–º —Ç–µ–∫—Å—Ç –∏–∑ –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä–∞
                        if not link_text or len(link_text) < 10:
                            if container:
                                # –ü—ã—Ç–∞–µ–º—Å—è –Ω–∞–π—Ç–∏ –∑–∞–≥–æ–ª–æ–≤–æ–∫
                                title_elem = container.find(['h1', 'h2', 'h3', 'h4', 'strong', 'b'])
                                if title_elem:
                                    link_text = title_elem.get_text(strip=True)
                                else:
                                    # –ë–µ—Ä–µ–º –ø–µ—Ä–≤—ã–µ 100 —Å–∏–º–≤–æ–ª–æ–≤ –∏–∑ –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä–∞
                                    container_text = container.get_text(strip=True)
                                    link_text = container_text[:100] if container_text else "–ë–µ–∑ –Ω–∞–∑–≤–∞–Ω–∏—è"
                        
                        description = ""
                        if container:
                            # –ü—ã—Ç–∞–µ–º—Å—è –Ω–∞–π—Ç–∏ –æ–ø–∏—Å–∞–Ω–∏–µ
                            desc_elem = container.find(['p', 'span', 'div'], recursive=True)
                            if desc_elem and desc_elem != link:
                                desc_text = desc_elem.get_text(strip=True)
                                # –ë–µ—Ä–µ–º —Ç–µ–∫—Å—Ç, –∫–æ—Ç–æ—Ä—ã–π —Å–æ–¥–µ—Ä–∂–∏—Ç –∫–ª—é—á–µ–≤–æ–µ —Å–ª–æ–≤–æ –∏–ª–∏ –Ω–∞—Ö–æ–¥–∏—Ç—Å—è —Ä—è–¥–æ–º
                                if keyword.lower() in desc_text.lower():
                                    description = desc_text[:300]
                                elif len(desc_text) > 20:
                                    description = desc_text[:300]
                        
                        announcement_data = {
                            'title': link_text[:200] if link_text else '–ë–µ–∑ –Ω–∞–∑–≤–∞–Ω–∏—è',
                            'url': full_url,
                            'matched_keywords': [keyword]
                        }
                        
                        if description and description != link_text:
                            announcement_data['description'] = description
                        
                        announcement_links.append(announcement_data)
                        seen_urls.add(full_url)
                        
                        logger.info(f"   ‚úÖ FALLBACK –Ω–∞—à–µ–ª: {link_text[:80]}...")
                        logger.debug(f"      URL: {full_url}")
                        if description:
                            logger.debug(f"      –û–ø–∏—Å–∞–Ω–∏–µ: {description[:100]}...")
                
                except Exception as e:
                    logger.debug(f"   ‚ö†Ô∏è –û—à–∏–±–∫–∞ –≤ fallback –¥–ª—è '{keyword}': {e}")
                    continue
        
        logger.info(f"   üìä FALLBACK —Ä–µ–∑—É–ª—å—Ç–∞—Ç: –Ω–∞–π–¥–µ–Ω–æ {len(announcement_links)} —Å—Å—ã–ª–æ–∫")
        return announcement_links[:10]
    
    def _extract_links_super_fallback(self, soup: BeautifulSoup, keywords: List[str], exchange_config: dict) -> List[Dict[str, str]]:
        """
        SUPER FALLBACK: –ü–æ—Å–ª–µ–¥–Ω–∏–π —à–∞–Ω—Å –Ω–∞–π—Ç–∏ —Å—Å—ã–ª–∫–∏.
        –ë–µ—Ä–µ—Ç –í–°–ï —Å—Å—ã–ª–∫–∏ –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ –∏ –ø—Ä–æ–≤–µ—Ä—è–µ—Ç, –µ—Å—Ç—å –ª–∏ —Ä—è–¥–æ–º –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞.
        """
        logger.info("   üöÄ SUPER FALLBACK: –ü—Ä–æ–≤–µ—Ä—è–µ–º –≤—Å–µ —Å—Å—ã–ª–∫–∏ –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ...")
        announcement_links = []
        seen_urls = set()
        
        # –ù–∞—Ö–æ–¥–∏–º –í–°–ï —Å—Å—ã–ª–∫–∏ –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ
        all_links = soup.find_all('a', href=True)
        logger.info(f"   üìä –ù–∞–π–¥–µ–Ω–æ {len(all_links)} —Å—Å—ã–ª–æ–∫ –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ")
        
        # –û–±—ä–µ–¥–∏–Ω—è–µ–º –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –≤ –æ–¥–Ω—É —Å—Ç—Ä–æ–∫—É –¥–ª—è –ø–æ–∏—Å–∫–∞
        keywords_lower = [kw.lower() for kw in keywords]
        
        for link in all_links:
            try:
                href = link.get('href', '')
                if not href or href.startswith('#') or href == '/':
                    continue
                
                # –§–æ—Ä–º–∏—Ä—É–µ–º –ø–æ–ª–Ω—ã–π URL
                if href.startswith('http'):
                    full_url = href
                elif href.startswith('/'):
                    url_prefix = exchange_config.get('url_prefix', '')
                    if url_prefix:
                        full_url = url_prefix + href
                    else:
                        from urllib.parse import urlparse
                        parsed = urlparse(self.url)
                        full_url = f"{parsed.scheme}://{parsed.netloc}{href}"
                else:
                    continue
                
                # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º —É–∂–µ –Ω–∞–π–¥–µ–Ω–Ω—ã–µ URL
                if full_url in seen_urls:
                    continue
                
                # –ü–æ–ª—É—á–∞–µ–º —Ç–µ–∫—Å—Ç —Å—Å—ã–ª–∫–∏
                link_text = link.get_text(strip=True)
                
                # –ò—â–µ–º —Å–∞–º—ã–π –±–æ–ª—å—à–æ–π —Ä–æ–¥–∏—Ç–µ–ª—å—Å–∫–∏–π –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä (–¥–æ 10 —É—Ä–æ–≤–Ω–µ–π)
                container = link
                for _ in range(10):
                    parent = container.find_parent(['div', 'article', 'section', 'li', 'tr', 'td'])
                    if parent:
                        container = parent
                    else:
                        break
                
                # –ü–æ–ª—É—á–∞–µ–º –≤–µ—Å—å —Ç–µ–∫—Å—Ç –∏–∑ –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä–∞
                container_text = container.get_text(separator=' ', strip=True).lower()
                
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —Å–æ–¥–µ—Ä–∂–∏—Ç –ª–∏ –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä —Ö–æ—Ç—è –±—ã –æ–¥–Ω–æ –∫–ª—é—á–µ–≤–æ–µ —Å–ª–æ–≤–æ
                matched_keywords = []
                for keyword in keywords:
                    if keyword.lower() in container_text:
                        matched_keywords.append(keyword)
                
                if matched_keywords:
                    # –ù–∞—à–ª–∏ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ! –ò–∑–≤–ª–µ–∫–∞–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é
                    
                    # –ü—ã—Ç–∞–µ–º—Å—è –Ω–∞–π—Ç–∏ –ª—É—á—à–∏–π –∑–∞–≥–æ–ª–æ–≤–æ–∫
                    title = link_text
                    if not title or len(title) < 10:
                        # –ò—â–µ–º –∑–∞–≥–æ–ª–æ–≤–æ–∫ –≤ —Ä–æ–¥–∏—Ç–µ–ª—è—Ö
                        header = container.find(['h1', 'h2', 'h3', 'h4', 'h5'])
                        if header:
                            title = header.get_text(strip=True)
                        else:
                            # –ë–µ—Ä–µ–º —Ç–µ–∫—Å—Ç –∏–∑ –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä–∞ (–ø–µ—Ä–≤—ã–µ 100 —Å–∏–º–≤–æ–ª–æ–≤)
                            title = container_text[:100]
                    
                    # –ò–∑–≤–ª–µ–∫–∞–µ–º –æ–ø–∏—Å–∞–Ω–∏–µ
                    description = ""
                    desc_elem = container.find(['p', 'div', 'span'])
                    if desc_elem:
                        desc_text = desc_elem.get_text(strip=True)
                        if len(desc_text) > 20:
                            description = desc_text[:300]
                    
                    announcement_data = {
                        'title': title[:200] if title else '–ë–µ–∑ –Ω–∞–∑–≤–∞–Ω–∏—è',
                        'url': full_url,
                        'matched_keywords': matched_keywords
                    }
                    
                    if description and description != title:
                        announcement_data['description'] = description
                    
                    announcement_links.append(announcement_data)
                    seen_urls.add(full_url)
                    
                    logger.info(f"   ‚úÖ SUPER FALLBACK –Ω–∞—à–µ–ª: {title[:80]}...")
                    logger.debug(f"      URL: {full_url}")
                    logger.debug(f"      –ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞: {', '.join(matched_keywords)}")
            
            except Exception as e:
                logger.debug(f"   ‚ö†Ô∏è –û—à–∏–±–∫–∞ –≤ super fallback: {e}")
                continue
        
        logger.info(f"   üìä SUPER FALLBACK —Ä–µ–∑—É–ª—å—Ç–∞—Ç: –Ω–∞–π–¥–µ–Ω–æ {len(announcement_links)} —Å—Å—ã–ª–æ–∫")
        return announcement_links[:10]

    def _clean_html(self, html: str) -> str:
        """
        –û—á–∏—Å—Ç–∫–∞ HTML –æ—Ç –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–∏—Ö —ç–ª–µ–º–µ–Ω—Ç–æ–≤ –ø–µ—Ä–µ–¥ —Ö—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ–º
        """
        # –£–¥–∞–ª—è–µ–º —Å–∫—Ä–∏–ø—Ç—ã –∏ —Å—Ç–∏–ª–∏
        soup = BeautifulSoup(html, 'html.parser')
        for script in soup(['script', 'style']):
            script.decompose()

        # –£–¥–∞–ª—è–µ–º –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–∏
        for comment in soup.findAll(text=lambda text: isinstance(text, str) and text.startswith('<!--')):
            comment.extract()

        # –ü–æ–ª—É—á–∞–µ–º –æ—á–∏—â–µ–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç
        clean_text = soup.get_text()

        # –£–¥–∞–ª—è–µ–º –ª–∏—à–Ω–∏–µ –ø—Ä–æ–±–µ–ª—ã
        clean_text = re.sub(r'\s+', ' ', clean_text).strip()

        return clean_text
