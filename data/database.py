# data/database.py
from sqlalchemy import create_engine, Index, text
from sqlalchemy.ext.declarative import declarative_base
from sqlalchemy.orm import sessionmaker, scoped_session
from sqlalchemy.pool import StaticPool
from contextlib import contextmanager
from datetime import datetime, timedelta
import logging
import threading

Base = declarative_base()

# –ò–º–ø–æ—Ä—Ç–∏—Ä—É–µ–º –º–æ–¥–µ–ª–∏ –ø–æ—Å–ª–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è Base
from data.models import *

# –ì–ª–æ–±–∞–ª—å–Ω—ã–µ –æ–±—ä–µ–∫—Ç—ã –ë–î
_engine = None
_SessionFactory = None
_lock = threading.RLock()

def init_database():
    """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è –∫ –ë–î —Å connection pooling"""
    global _engine, _SessionFactory
    
    with _lock:
        if _engine is not None:
            return
            
        database_url = 'sqlite:///data/database.db'
        
        # –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –¥–ª—è SQLite
        engine_kwargs = {
            'echo': False,
            'connect_args': {'check_same_thread': False}
        }
        
        engine_kwargs.update({
            'poolclass': StaticPool,
            'pool_pre_ping': True
        })
        
        _engine = create_engine(database_url, **engine_kwargs)
        _SessionFactory = sessionmaker(bind=_engine)
        
        # –°–æ–∑–¥–∞–µ–º —Ç–∞–±–ª–∏—Ü—ã
        create_tables()
        initialize_default_settings()
        
        logging.info("‚úÖ –ë–∞–∑–∞ –¥–∞–Ω–Ω—ã—Ö –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–∞")

@contextmanager
def get_db_session():
    """–ö–æ–Ω—Ç–µ–∫—Å—Ç–Ω—ã–π –º–µ–Ω–µ–¥–∂–µ—Ä –¥–ª—è —Å–µ—Å—Å–∏–π –ë–î"""
    session = None
    try:
        if _SessionFactory is None:
            init_database()
            
        session = _SessionFactory()
        yield session
        session.commit()
        
    except Exception as e:
        logging.error(f"‚ùå –û—à–∏–±–∫–∞ –≤ —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–∏ –ë–î: {e}")
        if session:
            session.rollback()
        raise
            
    finally:
        if session:
            session.close()

def get_db():
    """–£—Å—Ç–∞—Ä–µ–≤—à–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –æ–±—Ä–∞—Ç–Ω–æ–π —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏"""
    logging.warning("‚ö†Ô∏è –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è —É—Å—Ç–∞—Ä–µ–≤—à–∏–π get_db(), –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ get_db_session()")
    return _SessionFactory() if _SessionFactory else None

# –°–æ–∑–¥–∞–µ–º –∏–Ω–¥–µ–∫—Å—ã
def create_indexes():
    """–°–æ–∑–¥–∞–Ω–∏–µ –∏–Ω–¥–µ–∫—Å–æ–≤ –¥–ª—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏"""
    indexes = [
        Index('idx_proxy_status_speed', ProxyServer.status, ProxyServer.speed_ms),
        Index('idx_proxy_priority', ProxyServer.priority),
        Index('idx_ua_status_success', UserAgent.status, UserAgent.success_rate),
        Index('idx_stats_exchange_time', RotationStats.exchange, RotationStats.timestamp),
        Index('idx_stats_proxy_ua', RotationStats.proxy_id, RotationStats.user_agent_id),
        Index('idx_aggregated_date_exchange', AggregatedStats.date, AggregatedStats.exchange)
    ]
    return indexes

# –¢–†–ê–ù–ó–ê–ö–¶–ò–û–ù–ù–´–ï –û–ü–ï–†–ê–¶–ò–ò
@contextmanager
def transaction_session():
    """–ö–æ–Ω—Ç–µ–∫—Å—Ç–Ω—ã–π –º–µ–Ω–µ–¥–∂–µ—Ä –¥–ª—è —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–π —Å retry –ª–æ–≥–∏–∫–æ–π"""
    max_retries = 3
    retry_count = 0
    
    while retry_count < max_retries:
        with get_db_session() as session:
            try:
                yield session
                return
                
            except Exception as e:
                retry_count += 1
                logging.warning(f"‚ö†Ô∏è –ü–æ–≤—Ç–æ—Ä —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–∏ {retry_count}/{max_retries}: {e}")
                
                if retry_count == max_retries:
                    logging.error(f"‚ùå –¢—Ä–∞–Ω–∑–∞–∫—Ü–∏—è –ø—Ä–æ–≤–∞–ª–µ–Ω–∞ –ø–æ—Å–ª–µ {max_retries} –ø–æ–ø—ã—Ç–æ–∫")
                    raise

def atomic_operation(operation_func, *args, **kwargs):
    """–í—ã–ø–æ–ª–Ω–µ–Ω–∏–µ –æ–ø–µ—Ä–∞—Ü–∏–∏ –≤ —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–∏ —Å –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–º retry"""
    with transaction_session() as session:
        return operation_func(session, *args, **kwargs)

# –°–ò–°–¢–ï–ú–ê –ú–ò–ì–†–ê–¶–ò–ô
class DatabaseMigration:
    def __init__(self):
        self.migrations = []
        self._register_migrations()
    
    def _register_migrations(self):
        """–†–µ–≥–∏—Å—Ç—Ä–∞—Ü–∏—è –≤—Å–µ—Ö –º–∏–≥—Ä–∞—Ü–∏–π"""
        self.migrations.extend([
            self._migration_001_initial,
            self._migration_002_add_indexes,
            self._migration_003_add_multiple_urls,
            self._migration_004_convert_to_single_urls,
            self._migration_005_add_parsing_type,
            self._migration_006_add_staking_fields
        ])
    
    def _migration_001_initial(self, session):
        """–ü–µ—Ä–≤–æ–Ω–∞—á–∞–ª—å–Ω–∞—è –º–∏–≥—Ä–∞—Ü–∏—è"""
        pass
    
    def _migration_002_add_indexes(self, session):
        """–ú–∏–≥—Ä–∞—Ü–∏—è –¥–ª—è –¥–æ–±–∞–≤–ª–µ–Ω–∏—è –∏–Ω–¥–µ–∫—Å–æ–≤"""
        pass

    def _migration_003_add_multiple_urls(self, session):
        """–ú–∏–≥—Ä–∞—Ü–∏—è –¥–ª—è –¥–æ–±–∞–≤–ª–µ–Ω–∏—è –ø–æ–ª–µ–π api_urls –∏ html_urls"""
        try:
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –µ—Å—Ç—å –ª–∏ —É–∂–µ —ç—Ç–∏ —Å—Ç–æ–ª–±—Ü—ã
            result = session.execute(text("PRAGMA table_info(api_links)"))
            columns = [row[1] for row in result.fetchall()]

            # –î–æ–±–∞–≤–ª—è–µ–º api_urls –µ—Å–ª–∏ –µ–≥–æ –Ω–µ—Ç
            if 'api_urls' not in columns:
                session.execute(text("ALTER TABLE api_links ADD COLUMN api_urls TEXT DEFAULT '[]'"))
                logging.info("‚úÖ –î–æ–±–∞–≤–ª–µ–Ω —Å—Ç–æ–ª–±–µ—Ü api_urls")

            # –î–æ–±–∞–≤–ª—è–µ–º html_urls –µ—Å–ª–∏ –µ–≥–æ –Ω–µ—Ç
            if 'html_urls' not in columns:
                session.execute(text("ALTER TABLE api_links ADD COLUMN html_urls TEXT DEFAULT '[]'"))
                logging.info("‚úÖ –î–æ–±–∞–≤–ª–µ–Ω —Å—Ç–æ–ª–±–µ—Ü html_urls")

            session.commit()
        except Exception as e:
            logging.error(f"‚ùå –û—à–∏–±–∫–∞ –≤ –º–∏–≥—Ä–∞—Ü–∏–∏ 003: {e}")
            raise

    def _migration_004_convert_to_single_urls(self, session):
        """–ú–∏–≥—Ä–∞—Ü–∏—è 004: –ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö URL –≤ –æ–¥–∏–Ω–æ—á–Ω—ã–µ"""
        import json

        try:
            # –®–∞–≥ 1: –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö —Å—Ç–æ–ª–±—Ü–æ–≤
            result = session.execute(text("PRAGMA table_info(api_links)"))
            columns = [row[1] for row in result.fetchall()]

            # –®–∞–≥ 2: –î–æ–±–∞–≤–ª–µ–Ω–∏–µ –Ω–æ–≤—ã—Ö —Å—Ç–æ–ª–±—Ü–æ–≤
            if 'api_url' not in columns:
                session.execute(text("ALTER TABLE api_links ADD COLUMN api_url TEXT"))
                logging.info("‚úÖ –î–æ–±–∞–≤–ª–µ–Ω —Å—Ç–æ–ª–±–µ—Ü api_url")

            if 'html_url' not in columns:
                session.execute(text("ALTER TABLE api_links ADD COLUMN html_url TEXT"))
                logging.info("‚úÖ –î–æ–±–∞–≤–ª–µ–Ω —Å—Ç–æ–ª–±–µ—Ü html_url")

            session.commit()

            # –®–∞–≥ 3: –ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö –∏—Å–ø–æ–ª—å–∑—É—è raw SQL
            # –ü–æ–ª—É—á–∞–µ–º –¥–∞–Ω–Ω—ã–µ –Ω–∞–ø—Ä—è–º—É—é —á–µ—Ä–µ–∑ SQL
            result = session.execute(text("SELECT id, url, api_urls, html_urls, api_url, html_url, exchange FROM api_links"))
            rows = result.fetchall()
            converted_count = 0

            for row in rows:
                link_id = row[0]
                url = row[1]
                api_urls_json = row[2]
                html_urls_json = row[3]
                current_api_url = row[4]
                current_html_url = row[5]

                # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –µ—Å–ª–∏ —É–∂–µ –∫–æ–Ω–≤–µ—Ä—Ç–∏—Ä–æ–≤–∞–Ω–æ
                if current_api_url:
                    continue

                # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º API URLs
                new_api_url = None
                try:
                    api_urls_list = json.loads(api_urls_json) if api_urls_json else []
                    if api_urls_list:
                        new_api_url = api_urls_list[0]
                    elif url:
                        new_api_url = url
                except:
                    if url:
                        new_api_url = url

                # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º HTML URLs
                new_html_url = None
                try:
                    html_urls_list = json.loads(html_urls_json) if html_urls_json else []
                    if html_urls_list:
                        new_html_url = html_urls_list[0]
                except:
                    pass

                # –û–±–Ω–æ–≤–ª—è–µ–º —á–µ—Ä–µ–∑ SQL
                if new_api_url:
                    session.execute(text(
                        "UPDATE api_links SET api_url = :api_url, html_url = :html_url, exchange = NULL WHERE id = :id"
                    ), {"api_url": new_api_url, "html_url": new_html_url, "id": link_id})
                    converted_count += 1

            session.commit()
            logging.info(f"‚úÖ –ú–∏–≥—Ä–∞—Ü–∏—è 004: –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä–æ–≤–∞–Ω–æ {converted_count} —Å—Å—ã–ª–æ–∫")
            logging.info(f"   - JSON –º–∞—Å—Å–∏–≤—ã ‚Üí –æ–¥–∏–Ω–æ—á–Ω—ã–µ URL")
            logging.info(f"   - –ü–æ–ª–µ exchange –æ—á–∏—â–µ–Ω–æ")

        except Exception as e:
            logging.error(f"‚ùå –û—à–∏–±–∫–∞ –≤ –º–∏–≥—Ä–∞—Ü–∏–∏ 004: {e}")
            raise

    def _migration_005_add_parsing_type(self, session):
        """–ú–∏–≥—Ä–∞—Ü–∏—è 005: –î–æ–±–∞–≤–ª–µ–Ω–∏–µ –ø–æ–ª—è parsing_type"""
        try:
            # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö —Å—Ç–æ–ª–±—Ü–æ–≤
            result = session.execute(text("PRAGMA table_info(api_links)"))
            columns = [row[1] for row in result.fetchall()]

            # –î–æ–±–∞–≤–ª–µ–Ω–∏–µ –ø–æ–ª—è parsing_type
            if 'parsing_type' not in columns:
                session.execute(text("ALTER TABLE api_links ADD COLUMN parsing_type TEXT DEFAULT 'combined'"))
                logging.info("‚úÖ –î–æ–±–∞–≤–ª–µ–Ω —Å—Ç–æ–ª–±–µ—Ü parsing_type")
                session.commit()
            else:
                logging.info("‚ÑπÔ∏è –°—Ç–æ–ª–±–µ—Ü parsing_type —É–∂–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç")

        except Exception as e:
            logging.error(f"‚ùå –û—à–∏–±–∫–∞ –≤ –º–∏–≥—Ä–∞—Ü–∏–∏ 005: {e}")
            raise

    def _migration_006_add_staking_fields(self, session):
        """–ú–∏–≥—Ä–∞—Ü–∏—è 006: –î–æ–±–∞–≤–ª–µ–Ω–∏–µ –ø–æ–ª–µ–π –¥–ª—è —Å—Ç–µ–π–∫–∏–Ω–≥–∞"""
        try:
            # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö —Å—Ç–æ–ª–±—Ü–æ–≤
            result = session.execute(text("PRAGMA table_info(api_links)"))
            columns = [row[1] for row in result.fetchall()]

            # –î–æ–±–∞–≤–ª–µ–Ω–∏–µ –Ω–æ–≤—ã—Ö –ø–æ–ª–µ–π –¥–ª—è —Å—Ç–µ–π–∫–∏–Ω–≥–∞
            fields_to_add = {
                'category': "TEXT DEFAULT 'general'",
                'page_url': "TEXT",
                'min_apr': "REAL",
                'track_fill': "INTEGER DEFAULT 0",
                'statuses_filter': "TEXT",
                'types_filter': "TEXT"
            }

            added_count = 0
            for field_name, field_type in fields_to_add.items():
                if field_name not in columns:
                    session.execute(text(f"ALTER TABLE api_links ADD COLUMN {field_name} {field_type}"))
                    logging.info(f"‚úÖ –î–æ–±–∞–≤–ª–µ–Ω —Å—Ç–æ–ª–±–µ—Ü {field_name}")
                    added_count += 1

            if added_count > 0:
                session.commit()
                logging.info(f"‚úÖ –ú–∏–≥—Ä–∞—Ü–∏—è 006: –î–æ–±–∞–≤–ª–µ–Ω–æ {added_count} –ø–æ–ª–µ–π –¥–ª—è —Å—Ç–µ–π–∫–∏–Ω–≥–∞")
            else:
                logging.info("‚ÑπÔ∏è –í—Å–µ –ø–æ–ª—è —Å—Ç–µ–π–∫–∏–Ω–≥–∞ —É–∂–µ —Å—É—â–µ—Å—Ç–≤—É—é—Ç")

        except Exception as e:
            logging.error(f"‚ùå –û—à–∏–±–∫–∞ –≤ –º–∏–≥—Ä–∞—Ü–∏–∏ 006: {e}")
            raise

    def run_migrations(self):
        """–ó–∞–ø—É—Å–∫ –≤—Å–µ—Ö –º–∏–≥—Ä–∞—Ü–∏–π"""
        logging.info("üîÑ –ü—Ä–æ–≤–µ—Ä–∫–∞ –º–∏–≥—Ä–∞—Ü–∏–π –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö...")
        
        with get_db_session() as session:
            for i, migration in enumerate(self.migrations, 1):
                try:
                    migration(session)
                    logging.info(f"‚úÖ –ú–∏–≥—Ä–∞—Ü–∏—è {i}/{len(self.migrations)} –≤—ã–ø–æ–ª–Ω–µ–Ω–∞")
                except Exception as e:
                    logging.error(f"‚ùå –û—à–∏–±–∫–∞ –≤ –º–∏–≥—Ä–∞—Ü–∏–∏ {i}: {e}")
                    raise

# –û–ë–ù–û–í–õ–ï–ù–ù–´–ï –§–£–ù–ö–¶–ò–ò
def create_tables():
    """–°–æ–∑–¥–∞–Ω–∏–µ —Ç–∞–±–ª–∏—Ü —Å –æ–±—Ä–∞–±–æ—Ç–∫–æ–π –æ—à–∏–±–æ–∫"""
    try:
        Base.metadata.create_all(_engine)
        logging.info("‚úÖ –í—Å–µ —Ç–∞–±–ª–∏—Ü—ã —Å–æ–∑–¥–∞–Ω—ã/–ø—Ä–æ–≤–µ—Ä–µ–Ω—ã")
    except Exception as e:
        logging.error(f"‚ùå –û—à–∏–±–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è —Ç–∞–±–ª–∏—Ü: {e}")
        raise

def initialize_default_settings():
    """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –Ω–∞—Å—Ç—Ä–æ–µ–∫ –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é –≤ —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–∏"""
    def _init_settings(session):
        if not session.query(RotationSettings).first():
            default_settings = RotationSettings()
            session.add(default_settings)
            logging.info("‚úÖ –°–æ–∑–¥–∞–Ω—ã –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ —Ä–æ—Ç–∞—Ü–∏–∏ –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é")
    
    atomic_operation(_init_settings)

def cleanup_old_data():
    """–û—á–∏—Å—Ç–∫–∞ —Å—Ç–∞—Ä—ã—Ö –¥–∞–Ω–Ω—ã—Ö –≤ —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–∏"""
    def _cleanup(session):
        settings = session.query(RotationSettings).first()
        if not settings:
            return
            
        # –û—á–∏—Å—Ç–∫–∞ –¥–µ—Ç–∞–ª—å–Ω–æ–π —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏
        cutoff_date = datetime.utcnow() - timedelta(days=settings.stats_retention_days)
        deleted_stats = session.query(RotationStats).filter(
            RotationStats.timestamp < cutoff_date
        ).delete()
        
        # –ê—Ä—Ö–∏–≤–∞—Ü–∏—è –Ω–µ–∞–∫—Ç–∏–≤–Ω—ã—Ö –ø—Ä–æ–∫—Å–∏
        archive_cutoff = datetime.utcnow() - timedelta(days=settings.archive_inactive_days)
        
        archived_proxies = session.query(ProxyServer).filter(
            ProxyServer.status == 'inactive',
            ProxyServer.last_used < archive_cutoff,
            ProxyServer.archived_at.is_(None)
        ).update({
            'status': 'archived',
            'archived_at': datetime.utcnow()
        })
        
        archived_ua = session.query(UserAgent).filter(
            UserAgent.status == 'inactive', 
            UserAgent.last_used < archive_cutoff,
            UserAgent.archived_at.is_(None)
        ).update({
            'status': 'archived',
            'archived_at': datetime.utcnow()
        })
        
        settings.last_cleanup = datetime.utcnow()
        
        logging.info(f"üóëÔ∏è –û—á–∏—Å—Ç–∫–∞: —É–¥–∞–ª–µ–Ω–æ {deleted_stats} –∑–∞–ø–∏—Å–µ–π, "
                    f"–∞—Ä—Ö–∏–≤–∏—Ä–æ–≤–∞–Ω–æ {archived_proxies} –ø—Ä–æ–∫—Å–∏ –∏ {archived_ua} UA")
    
    atomic_operation(_cleanup)

# –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∞—è –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –ø—Ä–∏ –∏–º–ø–æ—Ä—Ç–µ
init_database()
migration_runner = DatabaseMigration()
migration_runner.run_migrations()